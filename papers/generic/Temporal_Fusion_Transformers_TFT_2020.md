# Temporal Fusion Transformers for Interpretable Multi-Horizon Time Series Forecasting (2020)

ğŸ“„ Paper Link: https://arxiv.org/abs/1912.09363  
ğŸ“‚ Local PDF: ../../1912.09363v3.pdf  
âœï¸ Authors: Lim et al.  
ğŸ« Published in: NeurIPS 2020  

---

## ğŸ¯ Objective
Propose a new Transformer-based architecture that effectively models **complex time series with multiple inputs** and provides **interpretability** through attention mechanisms and variable selection.

---

## ğŸ§  Key Contributions
- âœ… Introduces **Variable Selection Networks** for handling many features
- âœ… Integrates **static, known future, and historical inputs** effectively
- âœ… Provides **interpretability** via attention weights
- âœ… Achieves **state-of-the-art forecasting performance** on several datasets
- âœ… Designed specifically for **forecasting** (not NLP)

---

## ğŸ§ª Methodology
| Component | Purpose |
|----------|---------|
| LSTM Encoder | Learn temporal dynamics |
| Multi-Head Attention | Focus on relevant temporal patterns |
| Gating Mechanisms | Improve stability and prevent overfitting |
| Variable Selection | Identify most important features |

ğŸ“Œ Forecasting type: **multi-horizon**  
(ex.: next 24 steps)

Loss: Quantile loss â†’ supports **probabilistic forecasting**

---

## ğŸ“ˆ Results
- Outperforms models like:
  - DeepAR
  - LSTM
  - Seq2Seq
- Gains are strongest when **many features** influence the target

Interpretability examples:
- Feature importance scores
- Temporal attention heatmaps

---

## âœ… Strengths
- Designed for **real-world forecasting tasks**
- Handles **external/contextual variables** (calendar, locationâ€¦)
- Built-in **explainability**, helping decision-makers
- Better generalization with gating layers

---

## âš ï¸ Limitations
- More complex to train than basic Transformers/LSTMs
- Still suffers from **quadratic attention** complexity
- Interpretability can add computational overhead

---

## ğŸ¤ Relevance to CAST Project
âœ” Allows incorporation of **EV station metadata**  
âœ” Shows which variables matter most:
- ğŸš— usage patterns
- ğŸ•’ time of day / weekday
- ğŸ“ geolocation  
âœ” Useful for **availability prediction** at different horizons  
âœ” Good candidate to compare against Informer and basic Transformer

---

## ğŸ” Tags
Time Series â€¢ Transformers â€¢ Forecasting â€¢ Interpretability â€¢ EV Charging
