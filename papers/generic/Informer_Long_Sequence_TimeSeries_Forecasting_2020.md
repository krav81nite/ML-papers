# Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting (2020)

ğŸ“„ Paper Link: https://arxiv.org/abs/2012.07436  
ğŸ“‚ Local PDF: ../../2012.07436v3.pdf  
âœï¸ Authors: Zhou et al.  
ğŸ« Published in: AAAI 2021  

---

## ğŸ¯ Objective
Address the **quadratic complexity** of attention in Transformers and improve long sequence forecasting tasks through:

- **Efficient attention mechanisms**
- Better handling of **long-term dependencies**

---

## ğŸ§  Key Contributions
- âœ… **ProbSparse Self-Attention**  
  â†’ selects only *dominant queries*  
  â†’ reduces complexity from O(LÂ²) â O(L log L)
- âœ… **Self-Attention Distilling**  
  â†’ gradually compresses long sequences (pyramid-like)
- âœ… **Generative Decoder**  
  â†’ predicts full horizon in *one forward pass*

â¬†ï¸ This makes Informer **fast and scalable** for long-term forecasting.

---

## ğŸ§ª Methodology

| Component | Function |
|----------|----------|
| Encoder with distillation | Compress time dimension while keeping key signals |
| ProbSparse Attention | Efficient long-range pattern capture |
| Auto-regressive vs Generative outputs | Enables multi-step forecasting |

Framework goal:
ğŸ’¡ Preserve essential information while reducing compute.

Datasets used:
- ETT (Electricity Transformer Temperature)
- Weather
- Exchange
- Traffic

---

## ğŸ“ˆ Results

| Model | Long-Horizon Performance |
|-------|-------------------------|
| Informer | â­ Significant improvement vs LSTM/Transformer |
| Transforme
