# Long Short-Term Memory (1997)

ğŸ“„ Paper Link: https://www.bioinf.jku.at/publications/older/2604.pdf  
ğŸ“‚ Local PDF: ../../2604.pdf  
âœï¸ Authors: Hochreiter & Schmidhuber  
ğŸ« Published in: Neural Computation, 1997  

---

## ğŸ¯ Objective
Solve the **vanishing/exploding gradient problem** in RNNs, enabling learning of **long-term temporal dependencies** that standard recurrent networks fail to capture.

---

## ğŸ§  Key Contributions
- âœ… Proposes the **LSTM** architecture
- âœ… Introduces **memory cells** that preserve information over long time periods
- âœ… Introduces **input, output, and forget gates** to control memory flow
- âœ… Demonstrates improved learning stability in sequential tasks

---

## ğŸ§ª Methodology

| Component | Function |
|----------|----------|
| Memory Cell | Stores long-term state |
| Input Gate | Controls new information entering memory |
| Forget Gate | Removes outdated information |
| Output Gate | Produces hidden output |

Key equation form (simplified):

\[
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c_t}
\]

ğŸ’¡ The *forget gate* is crucial for sequence modeling.

---

## ğŸ“ˆ Results
Successfully learns tasks that standard RNNs **could not**:
- Long-distance temporal relationships
- Time-lag dependency benchmarks

Performance showed major advancements over:
- Vanilla RNNs
- Simple recurrent networks (SRNs)

---

## âœ… Strengths
- Solves gradient vanishing/exploding issues
- Excellent for **short to medium** forecasting horizons
- Very adaptable architecture

---

## âš ï¸ Limitations
- Still **sequential computation** â†’ slow training
- Struggles with **very long sequences**
- High parameter count â†’ comput
