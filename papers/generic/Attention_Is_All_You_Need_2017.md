# Attention Is All You Need (2017)

ğŸ“„ Paper Link: https://arxiv.org/abs/1706.03762  
ğŸ“‚ Local PDF: ../../1706.03762v7.pdf  
âœï¸ Authors: Vaswani et al.  
ğŸ« Published in: NeurIPS 2017  

---

## ğŸ¯ Objective
Introduce a novel neural network architecture for sequence modeling that **replaces recurrence** entirely using **self-attention**, achieving better parallelization and long-range dependency learning.

---

## ğŸ§  Key Contributions
- âœ… Introduced the **Transformer** architecture  
- âœ… Proposed **Multi-Head Self-Attention** mechanism  
- âœ… Removed recurrence â†’ **fully parallelizable**  
- âœ… Achieved state-of-the-art performance in machine translation  
- âœ… Laid the foundation for modern LLMs (BERT, GPT, T5â€¦)

---

## ğŸ§ª Methodology
| Component | Description |
|----------|-------------|
| Architecture | Encoderâ€“Decoder |
| Core Mechanism | Scaled Dot-Product Self-Attention |
| Positional Encoding | Injects notion of temporal order |
| Optimization | Adam with warm-up learning rate |

Key formula:
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

The model learns which parts of the sequence to **attend** to when predicting the next step.

---

## ğŸ“ˆ Results
- Outperformed previous **Seq2Seq with LSTM** in translation tasks
- Required **significantly less training time**
- Enabled modeling long contexts more efficiently than RNN-based models

| Model | BLEU Score | Training Speed |
|-------|------------|----------------|
| LSTM Seq2Seq | Lower | Slow |
| Transformer | â­ Higher | â­ Much Faster |

---

## âœ… Strengths
- Efficient for **long-range** sequence dependencies  
- Highly parallelizable â†’ faster inference and training  
- Flexible architecture â†’ adapts to many domains (text, time series, visionâ€¦)  

---

## âš ï¸ Limitations
- Complexity can grow **quadratically** with sequence length  
- Requires **positional encoding** to retain order  
- For very long time series â†’ may require optimized variants (ex.: Informer)  

---

## ğŸ¤ Relevance to CAST Project
âœ” Ideal for **predicting sequential patterns** in EV charging availability  
âœ” Can capture **daily/weekly cyclic usage**  
âœ” Enables adding **contextual inputs** (time of day, location, eventsâ€¦)  
âœ” A strong baseline to compare against LSTM and classical models  

---

## ğŸ” Tags
Transformers â€¢ Self-Attention â€¢ Sequence-to-Sequence â€¢ Deep Learning

